{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a433a0a",
   "metadata": {
    "papermill": {
     "duration": 0.004455,
     "end_time": "2025-04-06T13:05:03.193583",
     "exception": false,
     "start_time": "2025-04-06T13:05:03.189128",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Version 2 - Using Covisitation Matrices (Score 0.57845)\n",
    "In this notebook, we present a new approach for our recommendation system:\n",
    "We will use a data structure called a **Covisitation Matrix**.\n",
    "Covisitation Matrix captures the frequency with which pairs of items are interacted with together within user sessions. This matrix helps identify items that are often viewed, clicked, or purchased in close temporal proximity, enabling us to recommend items that are frequently co-engaged with others.\n",
    "\n",
    "In addition, we will also take into consideration popular items, and session lengths, when choosing the items to recommend.\n",
    "\n",
    "\n",
    "### Step 1 - Generate Candidates\n",
    "For each test session, we generate possible choices (candidates). In this notebook, we generate candidates from the following sources:\n",
    "* Session history of clicks, carts, orders\n",
    "* Most popular 20 clicks, carts, orders during test week\n",
    "* Co-visitaion matrices\n",
    "\n",
    "### Step 2 - ReRank and Choose 20\n",
    "Given the list of candidates (might be more than 20), we must select 20 to be our predictions.\n",
    "The rules give priority to:\n",
    "* Most recent previously visited items\n",
    "* Items previously visited multiple times\n",
    "* Items previously in cart or order\n",
    "* Co-visitation matrix of cart/order to cart/order\n",
    "* Popular items\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e323cb",
   "metadata": {
    "papermill": {
     "duration": 0.002969,
     "end_time": "2025-04-06T13:05:03.199900",
     "exception": false,
     "start_time": "2025-04-06T13:05:03.196931",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Step 1 - Candidate Generation with RAPIDS\n",
    "For candidate generation, we build four co-visitation matrices.\n",
    "\n",
    "* One computes the popularity of cart/order given a user's previous click/cart/order. We apply type weighting to this matrix.\n",
    "* One computes the popularity of cart/order given a user's previous cart/order. We call this \"buy2buy\" matrix.\n",
    "* One computes the popularity of clicks given a user previously click/cart/order.  We apply time weighting to this matrix.\n",
    "* One computes the popularity of cart/order given a user's previous click/cart/order, but only for interactions after 2 pm. (This matrix is already pre computed in this dataset: https://www.kaggle.com/datasets/cdeotte/otto-covisit-matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc798fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:05:03.207924Z",
     "iopub.status.busy": "2025-04-06T13:05:03.207521Z",
     "iopub.status.idle": "2025-04-06T13:05:05.886308Z",
     "shell.execute_reply": "2025-04-06T13:05:05.885330Z"
    },
    "papermill": {
     "duration": 2.685421,
     "end_time": "2025-04-06T13:05:05.888500",
     "exception": false,
     "start_time": "2025-04-06T13:05:03.203079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use RAPIDS version 21.10.01\n"
     ]
    }
   ],
   "source": [
    "VER = 5\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os, sys, pickle, glob, gc\n",
    "from collections import Counter\n",
    "import cudf, itertools\n",
    "print('We will use RAPIDS version',cudf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39541afa",
   "metadata": {
    "papermill": {
     "duration": 0.003277,
     "end_time": "2025-04-06T13:05:05.895476",
     "exception": false,
     "start_time": "2025-04-06T13:05:05.892199",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Compute the Co-visitation Matrices\n",
    "We compute the co-visitation matrices using RAPIDS cuDF on GPU, which is faster than using pandas on CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92382bd8",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-04-06T13:05:05.903833Z",
     "iopub.status.busy": "2025-04-06T13:05:05.903079Z",
     "iopub.status.idle": "2025-04-06T13:06:03.522416Z",
     "shell.execute_reply": "2025-04-06T13:06:03.521529Z"
    },
    "papermill": {
     "duration": 57.629179,
     "end_time": "2025-04-06T13:06:03.528025",
     "exception": false,
     "start_time": "2025-04-06T13:05:05.898846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will process 146 files, in groups of 5 and chunks of 25.\n",
      "CPU times: user 46.9 s, sys: 12.7 s, total: 59.6 s\n",
      "Wall time: 57.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CACHE FUNCTIONS\n",
    "def read_file(f):\n",
    "    return cudf.DataFrame( data_cache[f] )\n",
    "def read_file_to_cache(f):\n",
    "    df = pd.read_parquet(f)\n",
    "    df.ts = (df.ts/1000).astype('int32')\n",
    "    df['type'] = df['type'].map(type_labels).astype('int8')\n",
    "    return df\n",
    "\n",
    "# CACHE THE DATA ON CPU BEFORE PROCESSING ON GPU\n",
    "data_cache = {}\n",
    "type_labels = {'clicks':0, 'carts':1, 'orders':2}\n",
    "files = glob.glob('../input/otto-chunk-data-inparquet-format/*_parquet/*')\n",
    "for f in files: data_cache[f] = read_file_to_cache(f)\n",
    "\n",
    "# CHUNK PARAMETERS\n",
    "READ_CT = 5\n",
    "CHUNK = int( np.ceil( len(files)/6 ))\n",
    "print(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6329a2",
   "metadata": {
    "papermill": {
     "duration": 0.003321,
     "end_time": "2025-04-06T13:06:03.534913",
     "exception": false,
     "start_time": "2025-04-06T13:06:03.531592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1) \"Carts Orders\" Co-visitation Matrix - Type Weighted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05fa73fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:06:03.546586Z",
     "iopub.status.busy": "2025-04-06T13:06:03.546303Z",
     "iopub.status.idle": "2025-04-06T13:09:17.087994Z",
     "shell.execute_reply": "2025-04-06T13:09:17.086997Z"
    },
    "papermill": {
     "duration": 193.551506,
     "end_time": "2025-04-06T13:09:17.090038",
     "exception": false,
     "start_time": "2025-04-06T13:06:03.538532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 through 24 in groups of 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/cudf/core/frame.py:2600: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  \"When using a sequence of booleans for `ascending`, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 through 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 through 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 through 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 through 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 through 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 2\n",
      "Processing files 0 through 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 through 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 through 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 through 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 through 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 through 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 3\n",
      "Processing files 0 through 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 through 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 through 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 through 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 through 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 through 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 4\n",
      "Processing files 0 through 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 through 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 through 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 through 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 through 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 through 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "CPU times: user 2min 5s, sys: 1min 7s, total: 3min 12s\n",
      "Wall time: 3min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Computes co-visitation scores between products added to cart or ordered using session data.\n",
    "# For each product, it finds other products from the same session within 24 hours,\n",
    "# applies weights based on interaction type (click=1, cart=6, order=3),\n",
    "# and saves the top 15 most relevant product-to-product recommendations.\n",
    "\n",
    "type_weight = {0: 1, 1: 6, 2: 3}\n",
    "\n",
    "# Split data into manageable parts to avoid memory overflow\n",
    "DISK_PIECES = 4\n",
    "SIZE = 1.86e6 / DISK_PIECES  # Range size per disk part\n",
    "\n",
    "# Process in parts to handle large data efficiently\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART', PART + 1)\n",
    "\n",
    "    # Outer chunk loop: split full file list into groups of CHUNK size\n",
    "    for j in range(6):\n",
    "        start_file_idx = j * CHUNK\n",
    "        end_file_idx = min((j + 1) * CHUNK, len(files))\n",
    "        print(f'Processing files {start_file_idx} through {end_file_idx - 1} in groups of {READ_CT}...')\n",
    "\n",
    "        # Inner chunk loop: read and process files in groups of READ_CT\n",
    "        for k in range(start_file_idx, end_file_idx, READ_CT):\n",
    "            # Read a group of READ_CT files\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1, READ_CT):\n",
    "                if k + i < end_file_idx:\n",
    "                    df.append(read_file(files[k + i]))\n",
    "            df = cudf.concat(df, ignore_index=True, axis=0)\n",
    "\n",
    "            # Sort and keep only the last 30 interactions per session\n",
    "            df = df.sort_values(['session', 'ts'], ascending=[True, False])\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n < 30].drop('n', axis=1)\n",
    "\n",
    "            # Create product pairs from the same session within a 24-hour window\n",
    "            df = df.merge(df, on='session')\n",
    "            df = df.loc[((df.ts_x - df.ts_y).abs() < 24 * 60 * 60) & (df.aid_x != df.aid_y)]\n",
    "\n",
    "            # Filter by current disk part range to reduce memory usage\n",
    "            df = df.loc[(df.aid_x >= PART * SIZE) & (df.aid_x < (PART + 1) * SIZE)]\n",
    "\n",
    "            # Assign weights and aggregate\n",
    "            df = df[['session', 'aid_x', 'aid_y', 'type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = df.type_y.map(type_weight)\n",
    "            df = df[['aid_x', 'aid_y', 'wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x', 'aid_y']).wgt.sum()\n",
    "\n",
    "            # Combine results from inner chunks\n",
    "            if k == start_file_idx:\n",
    "                tmp2 = df\n",
    "            else:\n",
    "                tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k, ', ', end='')\n",
    "        print()\n",
    "\n",
    "        # Combine results from outer chunks\n",
    "        if start_file_idx == 0:\n",
    "            tmp = tmp2\n",
    "        else:\n",
    "            tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "\n",
    "    # Format and extract top 15 co-visitation scores per product\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x', 'wgt'], ascending=[True, False])\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n < 15].drop('n', axis=1)\n",
    "\n",
    "    # Save to disk (convert to pandas to reduce memory use)\n",
    "    tmp.to_pandas().to_parquet(f'top_15_carts_orders_v{VER}_{PART}.pqt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8309c0",
   "metadata": {
    "papermill": {
     "duration": 0.008604,
     "end_time": "2025-04-06T13:09:17.107688",
     "exception": false,
     "start_time": "2025-04-06T13:09:17.099084",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2) \"Buy2Buy\" Co-visitation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d25121ff",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-04-06T13:09:17.126102Z",
     "iopub.status.busy": "2025-04-06T13:09:17.125854Z",
     "iopub.status.idle": "2025-04-06T13:09:46.919356Z",
     "shell.execute_reply": "2025-04-06T13:09:46.918327Z"
    },
    "papermill": {
     "duration": 29.805168,
     "end_time": "2025-04-06T13:09:46.921336",
     "exception": false,
     "start_time": "2025-04-06T13:09:17.116168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/cudf/core/frame.py:2600: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  \"When using a sequence of booleans for `ascending`, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "CPU times: user 20.6 s, sys: 8.81 s, total: 29.4 s\n",
      "Wall time: 29.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Computes co-visitation scores between products added to cart or ordered (buy2buy).\n",
    "# Pairs are created from products in the same session within 14 days, then top 15 per product are saved.\n",
    "\n",
    "# Use the smallest possible number of disk pieces that avoids memory errors\n",
    "DISK_PIECES = 1\n",
    "SIZE = 1.86e6 / DISK_PIECES  # Range size per disk part\n",
    "\n",
    "# Process in parts to handle large data\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART', PART + 1)\n",
    "\n",
    "    # Outer chunk loop: divide full file list into CHUNK-sized groups\n",
    "    for j in range(6):\n",
    "        start_file_idx = j * CHUNK\n",
    "        end_file_idx = min((j + 1) * CHUNK, len(files))\n",
    "        print(f'Processing files {start_file_idx} thru {end_file_idx - 1} in groups of {READ_CT}...')\n",
    "\n",
    "        # Inner chunk loop: read and process files in groups of READ_CT\n",
    "        for k in range(start_file_idx, end_file_idx, READ_CT):\n",
    "            # Read a group of files\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1, READ_CT):\n",
    "                if k + i < end_file_idx:\n",
    "                    df.append(read_file(files[k + i]))\n",
    "            df = cudf.concat(df, ignore_index=True, axis=0)\n",
    "\n",
    "            # Keep only cart and order interactions\n",
    "            df = df.loc[df['type'].isin([1, 2])]\n",
    "\n",
    "            # Sort and keep only the last 30 interactions per session\n",
    "            df = df.sort_values(['session', 'ts'], ascending=[True, False])\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n < 30].drop('n', axis=1)\n",
    "\n",
    "            # Create product pairs from same session within 14 days\n",
    "            df = df.merge(df, on='session')\n",
    "            df = df.loc[((df.ts_x - df.ts_y).abs() < 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y)]\n",
    "\n",
    "            # Filter by current disk part range to manage memory\n",
    "            df = df.loc[(df.aid_x >= PART * SIZE) & (df.aid_x < (PART + 1) * SIZE)]\n",
    "\n",
    "            # Assign uniform weight and aggregate\n",
    "            df = df[['session', 'aid_x', 'aid_y', 'type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = 1\n",
    "            df = df[['aid_x', 'aid_y', 'wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x', 'aid_y']).wgt.sum()\n",
    "\n",
    "            # Combine results from inner chunks\n",
    "            if k == start_file_idx:\n",
    "                tmp2 = df\n",
    "            else:\n",
    "                tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k, ', ', end='')\n",
    "        print()\n",
    "\n",
    "        # Combine results from outer chunks\n",
    "        if start_file_idx == 0:\n",
    "            tmp = tmp2\n",
    "        else:\n",
    "            tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "\n",
    "    # Format and extract top 15 co-visitation scores per product\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x', 'wgt'], ascending=[True, False])\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n < 15].drop('n', axis=1)\n",
    "\n",
    "    # Save to disk (convert to pandas to reduce memory use)\n",
    "    tmp.to_pandas().to_parquet(f'top_15_buy2buy_v{VER}_{PART}.pqt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8404ddba",
   "metadata": {
    "papermill": {
     "duration": 0.010668,
     "end_time": "2025-04-06T13:09:46.942750",
     "exception": false,
     "start_time": "2025-04-06T13:09:46.932082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3) \"Clicks\" Co-visitation Matrix - Time Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "644df059",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-04-06T13:09:46.964489Z",
     "iopub.status.busy": "2025-04-06T13:09:46.964183Z",
     "iopub.status.idle": "2025-04-06T13:12:57.031176Z",
     "shell.execute_reply": "2025-04-06T13:12:57.030141Z"
    },
    "papermill": {
     "duration": 190.095223,
     "end_time": "2025-04-06T13:12:57.048310",
     "exception": false,
     "start_time": "2025-04-06T13:09:46.953087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 2\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 3\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 4\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "CPU times: user 2min 4s, sys: 1min 5s, total: 3min 10s\n",
      "Wall time: 3min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Computes co-visitation scores for product clicks.\n",
    "# For each product, finds other products clicked in the same session within 24 hours,\n",
    "# applies time-based weights, and saves the top 20 most relevant co-clicked products.\n",
    "\n",
    "# Use the smallest number of disk pieces that avoids memory errors\n",
    "DISK_PIECES = 4\n",
    "SIZE = 1.86e6 / DISK_PIECES  # Range size per disk part\n",
    "\n",
    "# Process in parts to handle large data efficiently\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART', PART + 1)\n",
    "\n",
    "    # Outer chunk loop: split full file list into CHUNK-sized groups\n",
    "    for j in range(6):\n",
    "        start_file_idx = j * CHUNK\n",
    "        end_file_idx = min((j + 1) * CHUNK, len(files))\n",
    "        print(f'Processing files {start_file_idx} thru {end_file_idx - 1} in groups of {READ_CT}...')\n",
    "\n",
    "        # Inner chunk loop: read and process files in groups of READ_CT\n",
    "        for k in range(start_file_idx, end_file_idx, READ_CT):\n",
    "            # Read a group of files\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1, READ_CT):\n",
    "                if k + i < end_file_idx:\n",
    "                    df.append(read_file(files[k + i]))\n",
    "            df = cudf.concat(df, ignore_index=True, axis=0)\n",
    "\n",
    "            # Sort and keep only the last 30 interactions per session\n",
    "            df = df.sort_values(['session', 'ts'], ascending=[True, False])\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n < 30].drop('n', axis=1)\n",
    "\n",
    "            # Create product pairs from same session within 24 hours\n",
    "            df = df.merge(df, on='session')\n",
    "            df = df.loc[((df.ts_x - df.ts_y).abs() < 24 * 60 * 60) & (df.aid_x != df.aid_y)]\n",
    "\n",
    "            # Filter by current disk part range to manage memory\n",
    "            df = df.loc[(df.aid_x >= PART * SIZE) & (df.aid_x < (PART + 1) * SIZE)]\n",
    "\n",
    "            # Assign time-decayed weights\n",
    "            df = df[['session', 'aid_x', 'aid_y', 'ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = 1 + 3 * (df.ts_x - 1659304800) / (1662328791 - 1659304800)\n",
    "            df = df[['aid_x', 'aid_y', 'wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x', 'aid_y']).wgt.sum()\n",
    "\n",
    "            # Combine results from inner chunks\n",
    "            if k == start_file_idx:\n",
    "                tmp2 = df\n",
    "            else:\n",
    "                tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k, ', ', end='')\n",
    "        print()\n",
    "\n",
    "        # Combine results from outer chunks\n",
    "        if start_file_idx == 0:\n",
    "            tmp = tmp2\n",
    "        else:\n",
    "            tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "\n",
    "    # Format and extract top 20 co-visitation scores per product\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x', 'wgt'], ascending=[True, False])\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n < 20].drop('n', axis=1)\n",
    "\n",
    "    # Save to disk (convert to pandas to reduce memory use)\n",
    "    tmp.to_pandas().to_parquet(f'top_20_clicks_v{VER}_{PART}.pqt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8d1e2e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:12:57.080003Z",
     "iopub.status.busy": "2025-04-06T13:12:57.079736Z",
     "iopub.status.idle": "2025-04-06T13:12:57.228482Z",
     "shell.execute_reply": "2025-04-06T13:12:57.227765Z"
    },
    "papermill": {
     "duration": 0.166788,
     "end_time": "2025-04-06T13:12:57.230290",
     "exception": false,
     "start_time": "2025-04-06T13:12:57.063502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FREE MEMORY\n",
    "del data_cache, tmp\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f67ea2",
   "metadata": {
    "papermill": {
     "duration": 0.015385,
     "end_time": "2025-04-06T13:12:57.261613",
     "exception": false,
     "start_time": "2025-04-06T13:12:57.246228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Step 2 - ReRank (choose 20) using handcrafted rules\n",
    "For description of the handcrafted rules, read this notebook's intro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd1f8bda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:12:57.293798Z",
     "iopub.status.busy": "2025-04-06T13:12:57.293510Z",
     "iopub.status.idle": "2025-04-06T13:12:58.394844Z",
     "shell.execute_reply": "2025-04-06T13:12:58.393873Z"
    },
    "papermill": {
     "duration": 1.119832,
     "end_time": "2025-04-06T13:12:58.396774",
     "exception": false,
     "start_time": "2025-04-06T13:12:57.276942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data has shape (6928123, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>aid</th>\n",
       "      <th>ts</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13099779</td>\n",
       "      <td>245308</td>\n",
       "      <td>1661795832</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13099779</td>\n",
       "      <td>245308</td>\n",
       "      <td>1661795862</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13099779</td>\n",
       "      <td>972319</td>\n",
       "      <td>1661795888</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13099779</td>\n",
       "      <td>972319</td>\n",
       "      <td>1661795898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13099779</td>\n",
       "      <td>245308</td>\n",
       "      <td>1661795907</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    session     aid          ts  type\n",
       "0  13099779  245308  1661795832     0\n",
       "1  13099779  245308  1661795862     1\n",
       "2  13099779  972319  1661795888     0\n",
       "3  13099779  972319  1661795898     1\n",
       "4  13099779  245308  1661795907     0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_test():    \n",
    "    dfs = []\n",
    "    # Iterate over all test parquet files in the specified directory\n",
    "    for e, chunk_file in enumerate(glob.glob('../input/otto-chunk-data-inparquet-format/test_parquet/*')):\n",
    "        chunk = pd.read_parquet(chunk_file)  # Load each chunk\n",
    "        chunk.ts = (chunk.ts / 1000).astype('int32')  # Convert timestamp from ms to seconds\n",
    "        chunk['type'] = chunk['type'].map(type_labels).astype('int8')  # Map interaction type to int8 label\n",
    "        dfs.append(chunk)  # Add to list of DataFrames\n",
    "\n",
    "    # Concatenate all test chunks into a single DataFrame\n",
    "    return pd.concat(dfs).reset_index(drop=True)  # .astype({\"ts\": \"datetime64[ms]\"}) optional\n",
    "\n",
    "# Load and inspect test data\n",
    "test_df = load_test()\n",
    "print('Test data has shape', test_df.shape)\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c4fdf76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:12:58.429620Z",
     "iopub.status.busy": "2025-04-06T13:12:58.429357Z",
     "iopub.status.idle": "2025-04-06T13:14:18.323207Z",
     "shell.execute_reply": "2025-04-06T13:14:18.322343Z"
    },
    "papermill": {
     "duration": 79.928224,
     "end_time": "2025-04-06T13:14:18.341169",
     "exception": false,
     "start_time": "2025-04-06T13:12:58.412945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 19s, sys: 5.93 s, total: 1min 25s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def pqt_to_dict(df):\n",
    "    return df.groupby('aid_x').aid_y.apply(list).to_dict()\n",
    "# LOAD THREE CO-VISITATION MATRICES\n",
    "top_20_clicks = pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_0.pqt') )\n",
    "for k in range(1,DISK_PIECES): \n",
    "    top_20_clicks.update( pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_{k}.pqt') ) )\n",
    "top_15_buys = pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_0.pqt') )\n",
    "for k in range(1,DISK_PIECES): \n",
    "    top_15_buys.update( pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_{k}.pqt') ) )\n",
    "top_20_buy2buy = pqt_to_dict( pd.read_parquet(f'top_15_buy2buy_v{VER}_0.pqt') )\n",
    "\n",
    "# TOP CLICKED AND ORDERED ITEMS IN THE TEST DATASET\n",
    "top_clicks = test_df.loc[test_df['type']=='clicks','aid'].value_counts().index.values[:20]\n",
    "top_orders = test_df.loc[test_df['type']=='orders','aid'].value_counts().index.values[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebdb0c8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:14:18.373813Z",
     "iopub.status.busy": "2025-04-06T13:14:18.373097Z",
     "iopub.status.idle": "2025-04-06T13:14:28.727994Z",
     "shell.execute_reply": "2025-04-06T13:14:28.727259Z"
    },
    "papermill": {
     "duration": 10.373633,
     "end_time": "2025-04-06T13:14:28.730158",
     "exception": false,
     "start_time": "2025-04-06T13:14:18.356525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Type weight mapping for different interaction types\n",
    "# clicks: 1, carts: 6, orders: 3\n",
    "type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n",
    "\n",
    "def suggest_clicks(df):\n",
    "    # Extract user history: item IDs and interaction types\n",
    "    aids = df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    \n",
    "    # Keep unique item IDs in reverse session order (most recent first)\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "\n",
    "    # If there are enough items in history, rerank based on weighted interactions\n",
    "    if len(unique_aids) >= 20:\n",
    "        weights = np.logspace(0.1, 1, len(aids), base=2, endpoint=True) - 1\n",
    "        aid_scores = Counter()\n",
    "        \n",
    "        # Score items by recency-weighted type importance\n",
    "        for aid, w, t in zip(aids, weights, types):\n",
    "            aid_scores[aid] += w * type_weight_multipliers[t]\n",
    "\n",
    "        # Return top 20 scored item IDs\n",
    "        sorted_aids = [k for k, v in aid_scores.most_common(20)]\n",
    "        return sorted_aids\n",
    "\n",
    "    # Otherwise, use co-visitation candidates from the click matrix\n",
    "    co_visitation_candidates = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n",
    "\n",
    "    # Rerank candidates by frequency, excluding already seen items\n",
    "    top_candidates = [aid for aid, cnt in Counter(co_visitation_candidates).most_common(20) if aid not in unique_aids]\n",
    "\n",
    "    # Combine unique user history with top candidates\n",
    "    result = unique_aids + top_candidates[:20 - len(unique_aids)]\n",
    "\n",
    "    # If still under 20, pad with global top clicked items\n",
    "    return result + list(top_clicks)[:20 - len(result)]\n",
    "\n",
    "\n",
    "# Load the top 40 co-visitation items after 2 PM from external file\n",
    "PATH = '/kaggle/input/otto-covisit-matrix/'\n",
    "top_40_day = pickle.load(open(PATH + '/top_40_aids_v181_0.pkl', 'rb'))\n",
    "\n",
    "def suggest_buys(df, custom_type_weights):\n",
    "    # Extract user history: item IDs and interaction types\n",
    "    aids = df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "\n",
    "    # Keep unique item IDs in reverse session order\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "\n",
    "    # Filter for cart/order interactions only to define \"buy\" items\n",
    "    df = df.loc[(df['type'] == 1) | (df['type'] == 2)]\n",
    "    unique_buys = list(dict.fromkeys(df.aid.tolist()[::-1]))\n",
    "\n",
    "    # If enough history exists, use weighted reranking\n",
    "    if len(unique_aids) >= 20:\n",
    "        weights = np.logspace(0.5, 1, len(aids), base=2, endpoint=True) - 1\n",
    "        aid_scores = Counter()\n",
    "\n",
    "        # Score items based on repeat interactions and type weights\n",
    "        for aid, w, t in zip(aids, weights, types):\n",
    "            aid_scores[aid] += w * custom_type_weights[t]\n",
    "\n",
    "        # Boost scores of items found in the buy2buy co-visitation matrix\n",
    "        buy2buy_candidates = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n",
    "        for aid in buy2buy_candidates:\n",
    "            aid_scores[aid] += 0.1\n",
    "\n",
    "        # Return top 20 scored item IDs\n",
    "        sorted_aids = [k for k, v in aid_scores.most_common(20)]\n",
    "        return sorted_aids\n",
    "\n",
    "    # Otherwise, use fallback co-visitation sources\n",
    "\n",
    "    # From cart/order co-visitation matrix\n",
    "    co_cart_order_candidates = list(itertools.chain(*[top_15_buys[aid] for aid in unique_aids if aid in top_15_buys]))\n",
    "\n",
    "    # From buy2buy co-visitation matrix\n",
    "    buy2buy_candidates = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n",
    "\n",
    "    # From time-of-day based co-visitation matrix (top 40 after 2 PM)\n",
    "    time_filtered_candidates = list(itertools.chain(*[top_40_day[aid][:10] for aid in unique_aids if aid in top_40_day]))\n",
    "\n",
    "    # Combine and rerank candidates by frequency, excluding items already seen\n",
    "    top_candidates = [aid for aid, cnt in Counter(\n",
    "        co_cart_order_candidates + buy2buy_candidates + time_filtered_candidates\n",
    "    ).most_common(20) if aid not in unique_aids]\n",
    "\n",
    "    # Combine history with top candidates\n",
    "    result = unique_aids + top_candidates[:20 - len(unique_aids)]\n",
    "\n",
    "    # If still under 20, pad with global top ordered items\n",
    "    return result + list(top_orders)[:20 - len(result)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ba39ba",
   "metadata": {
    "papermill": {
     "duration": 0.01537,
     "end_time": "2025-04-06T13:14:28.761777",
     "exception": false,
     "start_time": "2025-04-06T13:14:28.746407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create Submission CSV\n",
    "Inferring test data with Pandas groupby is slow. We need to accelerate the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3f9c26b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-06T13:14:28.793824Z",
     "iopub.status.busy": "2025-04-06T13:14:28.793372Z",
     "iopub.status.idle": "2025-04-06T13:59:37.507972Z",
     "shell.execute_reply": "2025-04-06T13:59:37.507030Z"
    },
    "papermill": {
     "duration": 2708.748586,
     "end_time": "2025-04-06T13:59:37.525768",
     "exception": false,
     "start_time": "2025-04-06T13:14:28.777182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45min 3s, sys: 9.38 s, total: 45min 12s\n",
      "Wall time: 45min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred_df_clicks = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n",
    "    lambda x: suggest_clicks(x)\n",
    ")\n",
    "\n",
    "cart_weights = {0: 1, 1: 6, 2: 3}\n",
    "order_weights = {0: 1, 1: 12, 2: 2}\n",
    "\n",
    "pred_df_carts = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n",
    "    lambda x: suggest_buys(x,cart_weights)\n",
    ")\n",
    "\n",
    "pred_df_orders = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n",
    "    lambda x: suggest_buys(x,order_weights)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9fda785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:59:37.558787Z",
     "iopub.status.busy": "2025-04-06T13:59:37.558467Z",
     "iopub.status.idle": "2025-04-06T13:59:40.406786Z",
     "shell.execute_reply": "2025-04-06T13:59:40.406045Z"
    },
    "papermill": {
     "duration": 2.867108,
     "end_time": "2025-04-06T13:59:40.408870",
     "exception": false,
     "start_time": "2025-04-06T13:59:37.541762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clicks_pred_df = pd.DataFrame(pred_df_clicks.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\n",
    "orders_pred_df = pd.DataFrame(pred_df_orders.add_suffix(\"_orders\"), columns=[\"labels\"]).reset_index()\n",
    "carts_pred_df = pd.DataFrame(pred_df_carts.add_suffix(\"_carts\"), columns=[\"labels\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6828f94e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:59:40.442225Z",
     "iopub.status.busy": "2025-04-06T13:59:40.441940Z",
     "iopub.status.idle": "2025-04-06T14:00:23.781059Z",
     "shell.execute_reply": "2025-04-06T14:00:23.780148Z"
    },
    "papermill": {
     "duration": 43.374083,
     "end_time": "2025-04-06T14:00:23.799142",
     "exception": false,
     "start_time": "2025-04-06T13:59:40.425059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12899779_clicks</td>\n",
       "      <td>59625 1253524 737445 438191 731692 1790770 942...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12899780_clicks</td>\n",
       "      <td>1142000 736515 973453 582732 1502122 889686 48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12899781_clicks</td>\n",
       "      <td>918667 199008 194067 57315 141736 1460571 7594...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12899782_clicks</td>\n",
       "      <td>834354 595994 740494 889671 987399 779477 1344...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12899783_clicks</td>\n",
       "      <td>1817895 607638 1754419 1216820 1729553 300127 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_type                                             labels\n",
       "0  12899779_clicks  59625 1253524 737445 438191 731692 1790770 942...\n",
       "1  12899780_clicks  1142000 736515 973453 582732 1502122 889686 48...\n",
       "2  12899781_clicks  918667 199008 194067 57315 141736 1460571 7594...\n",
       "3  12899782_clicks  834354 595994 740494 889671 987399 779477 1344...\n",
       "4  12899783_clicks  1817895 607638 1754419 1216820 1729553 300127 ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.concat([clicks_pred_df, orders_pred_df, carts_pred_df])\n",
    "pred_df.columns = [\"session_type\", \"labels\"]\n",
    "pred_df[\"labels\"] = pred_df.labels.apply(lambda x: \" \".join(map(str,x)))\n",
    "pred_df.to_csv(\"submission.csv\", index=False)\n",
    "pred_df.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 4493939,
     "sourceId": 38760,
     "sourceType": "competition"
    },
    {
     "datasetId": 2597726,
     "sourceId": 4436180,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2860873,
     "sourceId": 4933424,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30302,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3331.36868,
   "end_time": "2025-04-06T14:00:27.341860",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-06T13:04:55.973180",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
